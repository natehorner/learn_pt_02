{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPyQKT+zvuBU7VKqG1YSO9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natehorner/learn_pt_02/blob/main/pt_custom_data_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Stuff - once only downloads and imports"
      ],
      "metadata": {
        "id": "tix6dtnsoldG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KCJYjx9ojAa",
        "outputId": "6f539dce-7cd7-4aa8-fbae-6b1c5759cb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Torch version: 2.1.0+cu118\n",
            "data/pizza_steak_sushi directory doesn't exist, creating. and preparing data\n",
            "downloading pizza steak sushi data from github...\n",
            "Unzipping pizza steak sushi data\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "print(f\" Torch version: {torch.__version__}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "try:\n",
        "  import torchmetrics\n",
        "except:\n",
        "  !pip install -q torchmetrics\n",
        "  import torchmetrics\n",
        "#end try\n",
        "\n",
        "\n",
        "\n",
        "#download test data set\n",
        "use_subset = True\n",
        "if use_subset == True:\n",
        "  #create sub set\n",
        "  import requests\n",
        "  import zipfile\n",
        "  from pathlib import Path\n",
        "\n",
        "  #set up data folder\n",
        "  data_path = Path(\"data/\")\n",
        "  image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "  #if image folder doesn't exist, download it and prepare it\n",
        "  if image_path.is_dir():\n",
        "    print(f\"{image_path} directory already exists,no need to download\")\n",
        "  else:\n",
        "    print(f\"{image_path} directory doesn't exist, creating. and preparing data\")\n",
        "\n",
        "    #download\n",
        "    image_path.mkdir(parents=True,exist_ok=True)\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\",\"wb\") as f:\n",
        "      request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "      print(\"downloading pizza steak sushi data from github...\")\n",
        "      f.write(request.content)\n",
        "    #end with\n",
        "\n",
        "    #unzip data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\",\"r\") as zip_ref:\n",
        "      print(\"Unzipping pizza steak sushi data\")\n",
        "      zip_ref.extractall(image_path)\n",
        "else:\n",
        "  pass\n",
        "  \"\"\" not tested\n",
        "  train_data = datasets.Food101(\n",
        "      root=\"data\",#destination directory\n",
        "      train=True, #training, not test\n",
        "      download=True, #download yes\n",
        "      transform=ToTensor(), #how output should be handled\n",
        "      target_transform=None #how lables/targets should be handled\n",
        "  )\n",
        "\n",
        "  test_data = datasets.Food101(\n",
        "      root=\"data\",\n",
        "      train=False,\n",
        "      download=True,\n",
        "      transform=ToTensor(),\n",
        "      target_transform=None\n",
        "  )\n",
        "  \"\"\"\n",
        "#endif\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup - class and fn defs"
      ],
      "metadata": {
        "id": "f7oecMcuopVb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNwsa6rqox9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# understand the data set"
      ],
      "metadata": {
        "id": "9ghhtG2Uoyk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")\n",
        "  #end for\n",
        "#end def walk_through_dir()\n",
        "\n",
        "\n",
        "#set up training and test path\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "20:20:16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFdtVFOuwkG6",
        "outputId": "85a3818e-f106-438e-86ed-4cbce0ae708a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}